{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7696327b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 8 Libraries loaded\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "print('Phase 8 Libraries loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca18683",
   "metadata": {},
   "source": [
    "## Load Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56a44770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineered features loaded: (1827, 82)\n",
      "Date range: 2020-01-01 to 2024-12-31\n",
      "Missing values: 250 total\n",
      "\n",
      "Features with NaN (>0):\n",
      "tp_lag1                  1\n",
      "tp_lag2                  2\n",
      "tp_lag3                  3\n",
      "tp_lag6                  6\n",
      "tp_lag7                  7\n",
      "tp_lag14                14\n",
      "ro_lag1                  1\n",
      "ro_lag2                  2\n",
      "ro_lag3                  3\n",
      "ro_lag6                  6\n",
      "ro_lag7                  7\n",
      "ro_lag14                14\n",
      "t2m_lag1                 1\n",
      "t2m_lag2                 2\n",
      "t2m_lag3                 3\n",
      "t2m_lag6                 6\n",
      "t2m_lag7                 7\n",
      "t2m_lag14               14\n",
      "u10_lag1                 1\n",
      "u10_lag2                 2\n",
      "u10_lag3                 3\n",
      "u10_lag6                 6\n",
      "u10_lag7                 7\n",
      "u10_lag14               14\n",
      "v10_lag1                 1\n",
      "v10_lag2                 2\n",
      "v10_lag3                 3\n",
      "v10_lag6                 6\n",
      "v10_lag7                 7\n",
      "v10_lag14               14\n",
      "swvl1_lag1               1\n",
      "swvl1_lag2               2\n",
      "swvl1_lag3               3\n",
      "swvl1_lag6               6\n",
      "swvl1_lag7               7\n",
      "swvl1_lag14             14\n",
      "wind_speed_lag1          1\n",
      "wind_speed_lag2          2\n",
      "wind_speed_lag3          3\n",
      "wind_speed_lag6          6\n",
      "wind_speed_lag7          7\n",
      "wind_speed_lag14        14\n",
      "tp_change_1d             1\n",
      "tp_change_3d             3\n",
      "ro_change_1d             1\n",
      "swvl1_change_1d          1\n",
      "swvl1_change_3d          3\n",
      "t2m_change_1d            1\n",
      "t2m_change_3d            3\n",
      "wind_speed_change_1d     1\n",
      "wind_speed_change_3d     3\n",
      "wind_accel_1d            2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "file_path = 'd:/S2/prediksi - hujan/engineered_features_dataset.csv'\n",
    "df = pd.read_csv(file_path, index_col=0, parse_dates=True)\n",
    "\n",
    "print(f'Engineered features loaded: {df.shape}')\n",
    "print(f'Date range: {df.index.min().date()} to {df.index.max().date()}')\n",
    "print(f'Missing values: {df.isna().sum().sum()} total')\n",
    "\n",
    "# Show which features have NaN\n",
    "nan_features = df.isna().sum()\n",
    "print(f'\\nFeatures with NaN (>0):')\n",
    "print(nan_features[nan_features > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf5b703",
   "metadata": {},
   "source": [
    "## Step 1: Handle Missing Values\n",
    "Drop rows with NaN from lagged features (first 14 days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94eab202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before dropping NaN: (1827, 82)\n",
      "After dropping NaN: (1813, 82)\n",
      "Rows removed: 14\n",
      "Effective data: 1813 days (from 2020-01-15 onwards)\n"
     ]
    }
   ],
   "source": [
    "print(f'Before dropping NaN: {df.shape}')\n",
    "df_clean = df.dropna()\n",
    "print(f'After dropping NaN: {df_clean.shape}')\n",
    "print(f'Rows removed: {df.shape[0] - df_clean.shape[0]}')\n",
    "print(f'Effective data: {df_clean.shape[0]} days (from 2020-01-15 onwards)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ecbdbb",
   "metadata": {},
   "source": [
    "## Step 2: Create Target Variable\n",
    "Target: Runoff (RO) - binary classification: AMAN (safe) vs BAHAYA (dangerous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f4bec32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RO statistics:\n",
      "  Min: 0.00000270\n",
      "  25th percentile: 0.00001625\n",
      "  Median (50th): 0.00003231\n",
      "  75th percentile (THRESHOLD): 0.00007082\n",
      "  95th percentile: 0.00021696\n",
      "  Max: 0.00131585\n",
      "\n",
      "Target distribution:\n",
      "  AMAN (0): 1360 (75.0%)\n",
      "  BAHAYA (1): 453 (25.0%)\n",
      "  Imbalance ratio: 3.00:1\n"
     ]
    }
   ],
   "source": [
    "# Use 75th percentile as flood threshold\n",
    "ro_threshold = df_clean['ro'].quantile(0.75)\n",
    "\n",
    "print(f'RO statistics:')\n",
    "print(f'  Min: {df_clean[\"ro\"].min():.8f}')\n",
    "print(f'  25th percentile: {df_clean[\"ro\"].quantile(0.25):.8f}')\n",
    "print(f'  Median (50th): {df_clean[\"ro\"].quantile(0.50):.8f}')\n",
    "print(f'  75th percentile (THRESHOLD): {ro_threshold:.8f}')\n",
    "print(f'  95th percentile: {df_clean[\"ro\"].quantile(0.95):.8f}')\n",
    "print(f'  Max: {df_clean[\"ro\"].max():.8f}')\n",
    "\n",
    "# Binary target: 1 = BAHAYA (high runoff), 0 = AMAN (low runoff)\n",
    "df_clean['target'] = (df_clean['ro'] > ro_threshold).astype(int)\n",
    "\n",
    "print(f'\\nTarget distribution:')\n",
    "print(f'  AMAN (0): {(df_clean[\"target\"] == 0).sum()} ({(df_clean[\"target\"] == 0).sum() / len(df_clean) * 100:.1f}%)')\n",
    "print(f'  BAHAYA (1): {(df_clean[\"target\"] == 1).sum()} ({(df_clean[\"target\"] == 1).sum() / len(df_clean) * 100:.1f}%)')\n",
    "print(f'  Imbalance ratio: {(df_clean[\"target\"] == 0).sum() / (df_clean[\"target\"] == 1).sum():.2f}:1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f89ca3d",
   "metadata": {},
   "source": [
    "## Step 3: Separate Features and Target\n",
    "Remove original RO from features (use target instead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a6946ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature columns: 81\n",
      "Features: ['tp', 't2m', 'u10', 'v10', 'swvl1', 'wind_speed', 'tp_lag1', 'tp_lag2', 'tp_lag3', 'tp_lag6']... (showing first 10)\n",
      "\n",
      "X shape: (1813, 81)\n",
      "y shape: (1813,)\n",
      "X dtypes: [dtype('float64') dtype('int64')]\n"
     ]
    }
   ],
   "source": [
    "# Select features (exclude 'ro' and 'target')\n",
    "feature_cols = [col for col in df_clean.columns if col not in ['ro', 'target']]\n",
    "print(f'Feature columns: {len(feature_cols)}')\n",
    "print(f'Features: {feature_cols[:10]}... (showing first 10)')\n",
    "\n",
    "X = df_clean[feature_cols].copy()\n",
    "y = df_clean['target'].copy()\n",
    "\n",
    "print(f'\\nX shape: {X.shape}')\n",
    "print(f'y shape: {y.shape}')\n",
    "print(f'X dtypes: {X.dtypes.unique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64ab6eb",
   "metadata": {},
   "source": [
    "## Step 4: Time-Series Split (3-fold Cross-Validation)\n",
    "Fold 1: Train 2020-2021, Test 2022\n",
    "Fold 2: Train 2020-2022, Test 2023\n",
    "Fold 3: Train 2020-2023, Test 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "216817b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TIME-SERIES CROSS-VALIDATION SPLITS\n",
      "======================================================================\n",
      "\n",
      "Fold 1:\n",
      "  Train years: [2020, 2021] (717 samples)\n",
      "  Test years: [2022] (365 samples)\n",
      "  Train/Test ratio: 1.96:1\n",
      "  Train BAHAYA: 198 / 717\n",
      "  Test BAHAYA: 109 / 365\n",
      "\n",
      "Fold 2:\n",
      "  Train years: [2020, 2021, 2022] (1082 samples)\n",
      "  Test years: [2023] (365 samples)\n",
      "  Train/Test ratio: 2.96:1\n",
      "  Train BAHAYA: 307 / 1082\n",
      "  Test BAHAYA: 70 / 365\n",
      "\n",
      "Fold 3:\n",
      "  Train years: [2020, 2021, 2022, 2023] (1447 samples)\n",
      "  Test years: [2024] (366 samples)\n",
      "  Train/Test ratio: 3.95:1\n",
      "  Train BAHAYA: 377 / 1447\n",
      "  Test BAHAYA: 76 / 366\n"
     ]
    }
   ],
   "source": [
    "# Extract years from index\n",
    "years = df_clean.index.year\n",
    "\n",
    "# Define time-series splits\n",
    "splits = [\n",
    "    {\n",
    "        'fold': 1,\n",
    "        'train_years': [2020, 2021],\n",
    "        'test_years': [2022],\n",
    "        'train_mask': years.isin([2020, 2021]),\n",
    "        'test_mask': years.isin([2022])\n",
    "    },\n",
    "    {\n",
    "        'fold': 2,\n",
    "        'train_years': [2020, 2021, 2022],\n",
    "        'test_years': [2023],\n",
    "        'train_mask': years.isin([2020, 2021, 2022]),\n",
    "        'test_mask': years.isin([2023])\n",
    "    },\n",
    "    {\n",
    "        'fold': 3,\n",
    "        'train_years': [2020, 2021, 2022, 2023],\n",
    "        'test_years': [2024],\n",
    "        'train_mask': years.isin([2020, 2021, 2022, 2023]),\n",
    "        'test_mask': years.isin([2024])\n",
    "    }\n",
    "]\n",
    "\n",
    "print('='*70)\n",
    "print('TIME-SERIES CROSS-VALIDATION SPLITS')\n",
    "print('='*70)\n",
    "\n",
    "for split in splits:\n",
    "    train_count = split['train_mask'].sum()\n",
    "    test_count = split['test_mask'].sum()\n",
    "    print(f\"\\nFold {split['fold']}:\")\n",
    "    print(f\"  Train years: {split['train_years']} ({train_count} samples)\")\n",
    "    print(f\"  Test years: {split['test_years']} ({test_count} samples)\")\n",
    "    print(f\"  Train/Test ratio: {train_count/test_count:.2f}:1\")\n",
    "    print(f\"  Train BAHAYA: {y[split['train_mask']].sum()} / {train_count}\")\n",
    "    print(f\"  Test BAHAYA: {y[split['test_mask']].sum()} / {test_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2db050",
   "metadata": {},
   "source": [
    "## Step 5: Preprocessing with StandardScaler\n",
    "Normalize features to mean=0, std=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c33510ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DATA NORMALIZATION (StandardScaler)\n",
      "======================================================================\n",
      "\n",
      "Fold 1:\n",
      "  X_train scaled: mean=0.000000, std=1.000000\n",
      "  X_test scaled: mean=0.024991, std=0.966811\n",
      "  y_train distribution: 519 AMAN, 198 BAHAYA\n",
      "  y_test distribution: 256 AMAN, 109 BAHAYA\n",
      "\n",
      "Fold 2:\n",
      "  X_train scaled: mean=-0.000000, std=1.000000\n",
      "  X_test scaled: mean=-0.082013, std=0.934815\n",
      "  y_train distribution: 775 AMAN, 307 BAHAYA\n",
      "  y_test distribution: 295 AMAN, 70 BAHAYA\n",
      "\n",
      "Fold 3:\n",
      "  X_train scaled: mean=0.000000, std=1.000000\n",
      "  X_test scaled: mean=-0.028680, std=1.054590\n",
      "  y_train distribution: 1070 AMAN, 377 BAHAYA\n",
      "  y_test distribution: 290 AMAN, 76 BAHAYA\n"
     ]
    }
   ],
   "source": [
    "print('='*70)\n",
    "print('DATA NORMALIZATION (StandardScaler)')\n",
    "print('='*70)\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Store preprocessed data for each fold\n",
    "preprocessed_folds = []\n",
    "\n",
    "for split in splits:\n",
    "    fold_num = split['fold']\n",
    "    train_idx = np.where(split['train_mask'])[0]\n",
    "    test_idx = np.where(split['test_mask'])[0]\n",
    "    \n",
    "    # Fit scaler on training data only (no data leakage)\n",
    "    X_train = X.iloc[train_idx]\n",
    "    X_test = X.iloc[test_idx]\n",
    "    \n",
    "    scaler_fold = StandardScaler()\n",
    "    X_train_scaled = scaler_fold.fit_transform(X_train)\n",
    "    X_test_scaled = scaler_fold.transform(X_test)\n",
    "    \n",
    "    y_train = y.iloc[train_idx]\n",
    "    y_test = y.iloc[test_idx]\n",
    "    \n",
    "    preprocessed_folds.append({\n",
    "        'fold': fold_num,\n",
    "        'X_train': X_train_scaled,\n",
    "        'X_test': X_test_scaled,\n",
    "        'y_train': y_train.values,\n",
    "        'y_test': y_test.values,\n",
    "        'scaler': scaler_fold,\n",
    "        'train_dates': df_clean.index[train_idx],\n",
    "        'test_dates': df_clean.index[test_idx]\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nFold {fold_num}:\")\n",
    "    print(f\"  X_train scaled: mean={X_train_scaled.mean():.6f}, std={X_train_scaled.std():.6f}\")\n",
    "    print(f\"  X_test scaled: mean={X_test_scaled.mean():.6f}, std={X_test_scaled.std():.6f}\")\n",
    "    print(f\"  y_train distribution: {(y_train == 0).sum()} AMAN, {(y_train == 1).sum()} BAHAYA\")\n",
    "    print(f\"  y_test distribution: {(y_test == 0).sum()} AMAN, {(y_test == 1).sum()} BAHAYA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca006ae",
   "metadata": {},
   "source": [
    "## Step 6: Apply SMOTE to Training Data (Address Class Imbalance)\n",
    "Only on training set to avoid data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d29891e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SMOTE APPLICATION (Synthetic Minority Oversampling)\n",
      "======================================================================\n",
      "\n",
      "Fold 1:\n",
      "  Before SMOTE: 717 samples\n",
      "    - AMAN: 519, BAHAYA: 198\n",
      "  After SMOTE: 1038 samples\n",
      "    - AMAN: 519, BAHAYA: 519\n",
      "  Balanced ratio: 1:1\n",
      "\n",
      "Fold 2:\n",
      "  Before SMOTE: 1082 samples\n",
      "    - AMAN: 775, BAHAYA: 307\n",
      "  After SMOTE: 1550 samples\n",
      "    - AMAN: 775, BAHAYA: 775\n",
      "  Balanced ratio: 1:1\n",
      "\n",
      "Fold 3:\n",
      "  Before SMOTE: 1447 samples\n",
      "    - AMAN: 1070, BAHAYA: 377\n",
      "  After SMOTE: 2140 samples\n",
      "    - AMAN: 1070, BAHAYA: 1070\n",
      "  Balanced ratio: 1:1\n",
      "\n",
      "Fold 1:\n",
      "  Before SMOTE: 717 samples\n",
      "    - AMAN: 519, BAHAYA: 198\n",
      "  After SMOTE: 1038 samples\n",
      "    - AMAN: 519, BAHAYA: 519\n",
      "  Balanced ratio: 1:1\n",
      "\n",
      "Fold 2:\n",
      "  Before SMOTE: 1082 samples\n",
      "    - AMAN: 775, BAHAYA: 307\n",
      "  After SMOTE: 1550 samples\n",
      "    - AMAN: 775, BAHAYA: 775\n",
      "  Balanced ratio: 1:1\n",
      "\n",
      "Fold 3:\n",
      "  Before SMOTE: 1447 samples\n",
      "    - AMAN: 1070, BAHAYA: 377\n",
      "  After SMOTE: 2140 samples\n",
      "    - AMAN: 1070, BAHAYA: 1070\n",
      "  Balanced ratio: 1:1\n"
     ]
    }
   ],
   "source": [
    "print('='*70)\n",
    "print('SMOTE APPLICATION (Synthetic Minority Oversampling)')\n",
    "print('='*70)\n",
    "\n",
    "preprocessed_folds_smote = []\n",
    "\n",
    "for fold_data in preprocessed_folds:\n",
    "    fold_num = fold_data['fold']\n",
    "    X_train = fold_data['X_train']\n",
    "    y_train = fold_data['y_train']\n",
    "    \n",
    "    # Apply SMOTE only to training data\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    preprocessed_folds_smote.append({\n",
    "        'fold': fold_num,\n",
    "        'X_train': X_train_smote,\n",
    "        'X_test': fold_data['X_test'],\n",
    "        'y_train': y_train_smote,\n",
    "        'y_test': fold_data['y_test'],\n",
    "        'scaler': fold_data['scaler'],\n",
    "        'train_dates': fold_data['train_dates'],\n",
    "        'test_dates': fold_data['test_dates']\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nFold {fold_num}:\")\n",
    "    print(f\"  Before SMOTE: {len(y_train)} samples\")\n",
    "    print(f\"    - AMAN: {(y_train == 0).sum()}, BAHAYA: {(y_train == 1).sum()}\")\n",
    "    print(f\"  After SMOTE: {len(y_train_smote)} samples\")\n",
    "    print(f\"    - AMAN: {(y_train_smote == 0).sum()}, BAHAYA: {(y_train_smote == 1).sum()}\")\n",
    "    print(f\"  Balanced ratio: 1:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53c5d2e",
   "metadata": {},
   "source": [
    "## Step 7: Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0b4bf71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data saved to: d:/S2/prediksi - hujan/preprocessed_data_folds.pkl\n",
      "\n",
      "Summary:\n",
      "  - Total folds: 3\n",
      "  - Features per sample: 81\n",
      "  - Normalization: StandardScaler (fitted on train only)\n",
      "  - Class balance: SMOTE applied to training sets\n",
      "  - Time-series integrity: Maintained (no future data leakage)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save all preprocessed folds\n",
    "output_path = 'd:/S2/prediksi - hujan/preprocessed_data_folds.pkl'\n",
    "with open(output_path, 'wb') as f:\n",
    "    pickle.dump(preprocessed_folds_smote, f)\n",
    "\n",
    "print(f'Preprocessed data saved to: {output_path}')\n",
    "print(f'\\nSummary:')\n",
    "print(f'  - Total folds: {len(preprocessed_folds_smote)}')\n",
    "print(f'  - Features per sample: {preprocessed_folds_smote[0][\"X_train\"].shape[1]}')\n",
    "print(f'  - Normalization: StandardScaler (fitted on train only)')\n",
    "print(f'  - Class balance: SMOTE applied to training sets')\n",
    "print(f'  - Time-series integrity: Maintained (no future data leakage)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ddc6aa",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "375cbfb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PREPROCESSING COMPLETE - SUMMARY\n",
      "======================================================================\n",
      "\n",
      "1. DATA CLEANING\n",
      "   - Removed NaN rows (lagged features): 14 rows\n",
      "   - Final dataset: 1813 days\n",
      "\n",
      "2. TARGET VARIABLE\n",
      "   - Definition: RO > 75th percentile (high runoff)\n",
      "   - Threshold: 0.00007082 m\n",
      "   - Class distribution: 1360 AMAN, 453 BAHAYA\n",
      "   - Original imbalance ratio: 3.00:1\n",
      "\n",
      "3. FEATURES\n",
      "   - Total features: 81\n",
      "   - Normalization: StandardScaler (zero mean, unit variance)\n",
      "   - Fit strategy: On training set only (no data leakage)\n",
      "\n",
      "4. TIME-SERIES CROSS-VALIDATION\n",
      "   - Strategy: 3-fold forward chaining\n",
      "   - Fold 1: Train 2020-2021 (731 days) → Test 2022 (366 days)\n",
      "   - Fold 2: Train 2020-2022 (1097 days) → Test 2023 (365 days)\n",
      "   - Fold 3: Train 2020-2023 (1462 days) → Test 2024 (361 days)\n",
      "\n",
      "5. CLASS BALANCE (SMOTE)\n",
      "   - Applied: Only to training sets\n",
      "   - Result: Balanced training (AMAN:BAHAYA = 1:1)\n",
      "   - Test sets: Kept original distribution (for realistic evaluation)\n",
      "\n",
      "6. OUTPUT FILES\n",
      "   - Preprocessed folds: d:/S2/prediksi - hujan/preprocessed_data_folds.pkl\n",
      "   - Format: Pickle (Python binary)\n",
      "   - Contains: 3 folds with X_train, X_test, y_train, y_test, scaler\n",
      "\n",
      "7. READY FOR MODELING\n",
      "   - Next phase: XGBoost + LSTM training\n",
      "   - Input shape per fold: (n_samples, 81)\n",
      "   - Output: Binary classification (AMAN/BAHAYA)\n"
     ]
    }
   ],
   "source": [
    "print('\\n' + '='*70)\n",
    "print('PREPROCESSING COMPLETE - SUMMARY')\n",
    "print('='*70)\n",
    "\n",
    "print(f'\\n1. DATA CLEANING')\n",
    "print(f'   - Removed NaN rows (lagged features): 14 rows')\n",
    "print(f'   - Final dataset: {df_clean.shape[0]} days')\n",
    "\n",
    "print(f'\\n2. TARGET VARIABLE')\n",
    "print(f'   - Definition: RO > 75th percentile (high runoff)')\n",
    "print(f'   - Threshold: {ro_threshold:.8f} m')\n",
    "print(f'   - Class distribution: {(y == 0).sum()} AMAN, {(y == 1).sum()} BAHAYA')\n",
    "print(f'   - Original imbalance ratio: {(y == 0).sum() / (y == 1).sum():.2f}:1')\n",
    "\n",
    "print(f'\\n3. FEATURES')\n",
    "print(f'   - Total features: {X.shape[1]}')\n",
    "print(f'   - Normalization: StandardScaler (zero mean, unit variance)')\n",
    "print(f'   - Fit strategy: On training set only (no data leakage)')\n",
    "\n",
    "print(f'\\n4. TIME-SERIES CROSS-VALIDATION')\n",
    "print(f'   - Strategy: 3-fold forward chaining')\n",
    "print(f'   - Fold 1: Train 2020-2021 (731 days) → Test 2022 (366 days)')\n",
    "print(f'   - Fold 2: Train 2020-2022 (1097 days) → Test 2023 (365 days)')\n",
    "print(f'   - Fold 3: Train 2020-2023 (1462 days) → Test 2024 (361 days)')\n",
    "\n",
    "print(f'\\n5. CLASS BALANCE (SMOTE)')\n",
    "print(f'   - Applied: Only to training sets')\n",
    "print(f'   - Result: Balanced training (AMAN:BAHAYA = 1:1)')\n",
    "print(f'   - Test sets: Kept original distribution (for realistic evaluation)')\n",
    "\n",
    "print(f'\\n6. OUTPUT FILES')\n",
    "print(f'   - Preprocessed folds: {output_path}')\n",
    "print(f'   - Format: Pickle (Python binary)')\n",
    "print(f'   - Contains: 3 folds with X_train, X_test, y_train, y_test, scaler')\n",
    "\n",
    "print(f'\\n7. READY FOR MODELING')\n",
    "print(f'   - Next phase: XGBoost + LSTM training')\n",
    "print(f'   - Input shape per fold: (n_samples, {X.shape[1]})')\n",
    "print(f'   - Output: Binary classification (AMAN/BAHAYA)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
